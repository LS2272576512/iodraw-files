{"type":"wb","version":2,"source":"https://www.iodraw.com","elements":[{"id":"8Va62gtQNU3PQMFqRmPnl","type":"text","x":601,"y":429,"width":1427,"height":3497,"angle":0,"strokeColor":"#000000","backgroundColor":"transparent","fillStyle":"hachure","strokeWidth":1,"strokeStyle":"solid","roughness":1,"opacity":100,"groupIds":[],"strokeSharpness":"sharp","seed":1279738886,"version":1,"versionNonce":0,"isDeleted":false,"boundElements":null,"updated":1741252559577,"link":null,"text":"import cv2\r\nimport numpy as np\r\nimport mediapipe as mp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\nfrom sklearn.model_selection import train_test_split\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nmp_pose = mp.solutions.pose\r\npose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\r\n\r\n\r\ndef load_images(image_paths):\r\n    images = []\r\n    for path in image_paths:\r\n        img = cv2.imread(path)\r\n        img = cv2.resize(img, (256, 256))  \r\n        img = img / 255.0  \r\n        images.append(img)\r\n    return np.array(images)\r\n\r\n\r\ndef detect_pose(image):\r\n    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n    return results\r\n\r\n\r\ndef draw_landmarks(image, results):\r\n    if results.pose_landmarks:\r\n        mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\r\n    return image\r\n\r\nimage_paths = ['path_to_image1', 'path_to_image2', ...]  \r\nlabels = [0, 1, ...]  \r\nimages = load_images(image_paths)\r\nlabels = np.array(labels)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\r\n\r\n\r\nmodel = models.Sequential([\r\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Conv2D(64, (3, 3), activation='relu'),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Conv2D(128, (3, 3), activation='relu'),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Flatten(),\r\n    layers.Dense(128, activation='relu'),\r\n    layers.Dropout(0.5),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\r\n\r\n\r\ntest_loss, test_acc = model.evaluate(X_test, y_test)\r\nprint(f\"Test accuracy: {test_acc}\")\r\n\r\nmodel.save('advanced_boxing_model.h5')\r\n\r\n\r\ndef predict_punch(image_path):\r\n    img = cv2.imread(image_path)\r\n    img = cv2.resize(img, (256, 256))\r\n    img = img / 255.0\r\n    img = np.expand_dims(img, axis=0)\r\n    prediction = model.predict(img)\r\n    return prediction\r\n\r\n\r\ndef analyze_motion(video_path):\r\n    cap = cv2.VideoCapture(video_path)\r\n    trajectories = []\r\n\r\n    while cap.isOpened():\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            break\r\n\r\n\r\n        results = detect_pose(frame)\r\n        if results.pose_landmarks:\r\n\r\n            landmarks = results.pose_landmarks.landmark\r\n\r\n            right_hand = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\r\n            trajectories.append(right_hand)\r\n\r\n\r\n        frame = draw_landmarks(frame, results)\r\n        cv2.imshow('Frame', frame)\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\n\r\n    trajectories = np.array(trajectories)\r\n    plt.plot(trajectories[:, 0], trajectories[:, 1], label='Right Hand Trajectory')\r\n    plt.xlabel('X Position')\r\n    plt.ylabel('Y Position')\r\n    plt.legend()\r\n    plt.show()\r\n\r\n    return trajectories\r\n\r\n\r\ndef provide_feedback(prediction, trajectories):\r\n    if prediction > 0.5:\r\n        print(\"Incorrect punch detected.\")\r\n\r\n        if np.std(trajectories[:, 0]) > 0.1:\r\n            print(\"Try to stabilize your horizontal movement.\")\r\n        if np.std(trajectories[:, 1]) > 0.1:\r\n            print(\"Try to stabilize your vertical movement.\")\r\n    else:\r\n        print(\"Correct punch detected. Keep up the good work!\")\r\n\r\n\r\nimage_path = 'path_to_new_image'\r\nvideo_path = 'path_to_video'\r\n\r\nprediction = predict_punch(image_path)\r\ntrajectories = analyze_motion(video_path)\r\nprovide_feedback(prediction, trajectories)\r\n\r\nplt.plot(history.history['accuracy'], label='accuracy')\r\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Accuracy')\r\nplt.ylim([0, 1])\r\nplt.legend(loc='lower right')\r\nplt.show()","fontSize":20,"fontFamily":1,"textAlign":"left","verticalAlign":"top","baseline":3490,"containerId":null,"originalText":"import cv2\r\nimport numpy as np\r\nimport mediapipe as mp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\nfrom sklearn.model_selection import train_test_split\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nmp_pose = mp.solutions.pose\r\npose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\r\n\r\n\r\ndef load_images(image_paths):\r\n    images = []\r\n    for path in image_paths:\r\n        img = cv2.imread(path)\r\n        img = cv2.resize(img, (256, 256))  \r\n        img = img / 255.0  \r\n        images.append(img)\r\n    return np.array(images)\r\n\r\n\r\ndef detect_pose(image):\r\n    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n    return results\r\n\r\n\r\ndef draw_landmarks(image, results):\r\n    if results.pose_landmarks:\r\n        mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\r\n    return image\r\n\r\nimage_paths = ['path_to_image1', 'path_to_image2', ...]  \r\nlabels = [0, 1, ...]  \r\nimages = load_images(image_paths)\r\nlabels = np.array(labels)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\r\n\r\n\r\nmodel = models.Sequential([\r\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Conv2D(64, (3, 3), activation='relu'),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Conv2D(128, (3, 3), activation='relu'),\r\n    layers.MaxPooling2D((2, 2)),\r\n    layers.Flatten(),\r\n    layers.Dense(128, activation='relu'),\r\n    layers.Dropout(0.5),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\r\n\r\n\r\ntest_loss, test_acc = model.evaluate(X_test, y_test)\r\nprint(f\"Test accuracy: {test_acc}\")\r\n\r\nmodel.save('advanced_boxing_model.h5')\r\n\r\n\r\ndef predict_punch(image_path):\r\n    img = cv2.imread(image_path)\r\n    img = cv2.resize(img, (256, 256))\r\n    img = img / 255.0\r\n    img = np.expand_dims(img, axis=0)\r\n    prediction = model.predict(img)\r\n    return prediction\r\n\r\n\r\ndef analyze_motion(video_path):\r\n    cap = cv2.VideoCapture(video_path)\r\n    trajectories = []\r\n\r\n    while cap.isOpened():\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            break\r\n\r\n\r\n        results = detect_pose(frame)\r\n        if results.pose_landmarks:\r\n\r\n            landmarks = results.pose_landmarks.landmark\r\n\r\n            right_hand = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\r\n            trajectories.append(right_hand)\r\n\r\n\r\n        frame = draw_landmarks(frame, results)\r\n        cv2.imshow('Frame', frame)\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\n\r\n    trajectories = np.array(trajectories)\r\n    plt.plot(trajectories[:, 0], trajectories[:, 1], label='Right Hand Trajectory')\r\n    plt.xlabel('X Position')\r\n    plt.ylabel('Y Position')\r\n    plt.legend()\r\n    plt.show()\r\n\r\n    return trajectories\r\n\r\n\r\ndef provide_feedback(prediction, trajectories):\r\n    if prediction > 0.5:\r\n        print(\"Incorrect punch detected.\")\r\n\r\n        if np.std(trajectories[:, 0]) > 0.1:\r\n            print(\"Try to stabilize your horizontal movement.\")\r\n        if np.std(trajectories[:, 1]) > 0.1:\r\n            print(\"Try to stabilize your vertical movement.\")\r\n    else:\r\n        print(\"Correct punch detected. Keep up the good work!\")\r\n\r\n\r\nimage_path = 'path_to_new_image'\r\nvideo_path = 'path_to_video'\r\n\r\nprediction = predict_punch(image_path)\r\ntrajectories = analyze_motion(video_path)\r\nprovide_feedback(prediction, trajectories)\r\n\r\nplt.plot(history.history['accuracy'], label='accuracy')\r\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Accuracy')\r\nplt.ylim([0, 1])\r\nplt.legend(loc='lower right')\r\nplt.show()"}],"appState":{"theme":"light","currentChartType":"bar","currentItemBackgroundColor":"transparent","currentItemEndArrowhead":"arrow","currentItemFillStyle":"hachure","currentItemFontFamily":1,"currentItemFontSize":20,"currentItemLinearStrokeSharpness":"round","currentItemOpacity":100,"currentItemRoughness":1,"currentItemStartArrowhead":null,"currentItemStrokeColor":"#000000","currentItemStrokeSharpness":"sharp","currentItemStrokeStyle":"solid","currentItemStrokeWidth":1,"currentItemTextAlign":"left","cursorButton":"up","editingGroupId":null,"elementLocked":false,"elementType":"selection","exportBackground":true,"exportScale":1,"exportEmbedScene":false,"exportWithDarkMode":false,"gridSize":null,"lastPointerDownWith":"mouse","name":"Untitled-2025-03-06-1715","openMenu":null,"previousSelectedElementIds":{"8Va62gtQNU3PQMFqRmPnl":true},"scrolledOutside":false,"scrollX":0,"scrollY":0,"selectedElementIds":{"Ylf0HOROPb-JhvdE0n0ZJ":true},"selectedGroupIds":{},"shouldCacheIgnoreZoom":false,"showStats":false,"viewBackgroundColor":"#ffffff","zenModeEnabled":false,"zoom":{"value":1}},"files":{}}